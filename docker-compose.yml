version: '3.8'

services:
    nifi:
        image: 'apache/nifi:1.24.0'
        container_name: nifi_bd
        hostname: nifi_bd
        environment:
            - NIFI_WEB_HTTP_PORT=8080
            - SINGLE_USER_CREDENTIALS_USERNAME=admin
            - SINGLE_USER_CREDENTIALS_PASSWORD=S3curePa55word
            - NIFI_SENSITIVE_PROPS_KEY=pUaEVgyGKT61fMCAWNbjJPMwAcQDuDj4
            - WARSAW_API_KEY=${WARSAW_API_KEY}
            - WEATHER_API_KEY=${WEATHER_API_KEY}
        volumes:
            - './src/nifi/conf:/opt/nifi/nifi-current/conf'
        ports:
            - '8080:8080' # Nifi web UI
        networks:
            - big-data-network

    zookeeper:
        image: wurstmeister/zookeeper:latest
        container_name: zookeeper_bd
        hostname: zookeeper_bd
        environment:
            ZOOKEEPER_CLIENT_PORT: 2181
            ZOOKEEPER_TICK_TIME: 2000
        ports:
            - '2181:2181'
        networks:
            - big-data-network

    kafka:
        image: wurstmeister/kafka:latest
        container_name: kafka_bd
        hostname: kafka_bd
        depends_on:
            - zookeeper
        environment:
            KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka_bd:9093,OUTSIDE://localhost:9092
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
            KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
            KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
            KAFKA_ZOOKEEPER_CONNECT: zookeeper_bd:2181
            KAFKA_CREATE_TOPICS: "my-topic:1:1, transport-location:1:1, weather:1:1, stops:1:1"
        ports:
            - '9092:9092'
        expose:
            - "9093"
        networks:
            - big-data-network

    spark-structured-streaming:
        image: docker.io/bitnami/spark:3
        container_name: spark_structured_streaming_master_bd
        hostname: spark_structured_streaming_master_bd
        user: root
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        ports:
            - '8081:8080' # Spark web UI
        volumes:
            - ./src/spark:/home
        networks:
            - big-data-network

    hdfs-namenode:
        image: gradiant/hdfs-namenode
        container_name: hdfs-namenode
        hostname: hdfs-namenode
        ports:
            - '50070:50070'  # NameNode web UI
            - '8020:8020'    # HDFS RPC port
        networks:
            - big-data-network

    hdfs-datanode1:
        image: gradiant/hdfs-datanode
        container_name: hdfs-datanode1
        hostname: hdfs-datanode1
        links:
            - hdfs-namenode
        environment:
            - 'CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:8020'
        networks:
            - big-data-network

    hdfs-datanode2:
        image: gradiant/hdfs-datanode
        container_name: hdfs-datanode2
        hostname: hdfs-datanode2
        links:
            - hdfs-namenode
        environment:
            - 'CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:8020'
        networks:
            - big-data-network

    redis:
        image: redis
        container_name: redis_bd
        hostname: redis_bd
        ports:
            - '6379:6379'
        networks:
            - big-data-network
 
    # spark:
    #     image: bitnami/spark:3.3.0
    #     container_name: spark_test
    #     hostname: spark_test
    #     environment:
    #         - SPARK_MODE=master    # Single-node, Spark in master mode
    #         - SPARK_MASTER_URL=spark://spark_test:7077  # Local Spark master
    #         - SPARK_DRIVER_MEMORY=2g
    #         - SPARK_EXECUTOR_MEMORY=2g
    #         - SPARK_EXECUTOR_CORES=2
    #     ports:
    #         - "4040:4040"   # Spark UI port for monitoring
    #         - "7077:7077"   # Spark master port
    #     volumes:
    #         - ./spark:/opt/spark  # Local Spark directory for your code and configurations
    #     networks:
    #         - big-data-network
    #     command: ["start-master.sh", "start-worker.sh", "spark://spark_test:7077"]


networks:
    big-data-network:
        driver: bridge